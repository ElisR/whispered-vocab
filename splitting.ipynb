{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding audio files\n",
    "import glob\n",
    "\n",
    "# Manipulating audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "from pydub.playback import play\n",
    "\n",
    "# Generating Anki deck\n",
    "import genanki\n",
    "import random\n",
    "\n",
    "# Modifying strings\n",
    "import re\n",
    "\n",
    "# Machine Learning Model\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "# Saving output\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "# Checking confidence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all mp3 files\n",
    "dir = \"french_audio/split_testing/\"\n",
    "filename_dir_list = sorted(glob.glob(dir + \"*.mp3\"))\n",
    "filename_only_list = [ f_dir.split(\"/\")[-1] for f_dir in filename_dir_list ]\n",
    "\n",
    "# Choose file\n",
    "file_number = 1\n",
    "filename = filename_only_list[file_number]\n",
    "\n",
    "# TODO Fix to be more generic\n",
    "filename_prefix = filename[:4]\n",
    "filename_title = filename[5:-4]\n",
    "\n",
    "track = AudioSegment.from_mp3(dir + filename)\n",
    "original_bitrate = mediainfo(dir + filename)[\"bit_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters empirically tuned\n",
    "# Discard first two chunks because they're English\n",
    "chunks = split_on_silence(track, min_silence_len=600, silence_thresh=-30, keep_silence=200)[2:]\n",
    "\n",
    "def chunk_filename(filename_prefix, i):\n",
    "    return filename_prefix + \"_\" + str(i) + \".mp3\"\n",
    "\n",
    "chunk_filename_list = [ chunk_filename(filename_prefix, i) for i in range(len(chunks)) ]\n",
    "\n",
    "# Save split up audio\n",
    "for i, (chunk, chunk_name) in enumerate(zip(chunks, chunk_filename_list)):\n",
    "    # Only write if filename doesn't exist\n",
    "    if not os.path.isfile(chunk_name):\n",
    "        chunk.export(chunk_name, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Apply to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23e13b2b61648df8c67a1aa98cb8f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phrases_fr = []\n",
    "phrases_en = []\n",
    "\n",
    "logprobs_fr = []\n",
    "logprobs_en = []\n",
    "\n",
    "# Has to be changed if on CPU vs GPU\n",
    "decode_options = {\"fp16\": True}\n",
    "\n",
    "num_chunks = len(chunks)\n",
    "for i in tqdm(range(num_chunks)):\n",
    "    # Ignoring the English title\n",
    "    result_fr = model.transcribe(chunk_filename(filename_prefix, i), language=\"fr\", task=\"transcribe\", **decode_options)\n",
    "    result_en = model.transcribe(chunk_filename(filename_prefix, i), language=\"fr\", task=\"translate\", **decode_options)\n",
    "\n",
    "    # Saving text\n",
    "    phrases_fr.append(result_fr[\"text\"])\n",
    "    phrases_en.append(result_en[\"text\"])\n",
    "\n",
    "    # Saving confidence\n",
    "    logprobs_fr.append(result_fr[\"segments\"][0][\"avg_logprob\"])\n",
    "    logprobs_en.append(result_en[\"segments\"][0][\"avg_logprob\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('un passeport', 'a passport'), (\"les papiers d'identité\", 'identity papers'), (\"une carte d'identité\", 'an identity card'), ('un continent', 'a continent'), ('a bie!', 'a b'), ('une nation', 'a nation'), ('la nationalité', 'Nationality'), ('fatima va demander la nationalité française', 'fatima will ask French nationality'), (\"être d'origine\", 'being of origin'), (\"c'est un français d'origine italienne\", \"it's a Frenchman of Italian origin\"), ('étranger', 'Stranger'), ('étranger', 'Overseas'), ('un étranger', 'a stranger'), ('une étrangère', 'a foreigner'), ('Immigrés', 'immigrate'), ('un immigré', 'an immigrant'), ('une immigrée', 'an immigrant'), (\"les immigrés ont parfois du mal à s'intégrer\", 'immigrants sometimes have trouble integrating themselves'), (\"l'immigration\", 'Immigration'), ('Émigrez', 'emigrate'), ('une langue', 'a language'), ('la langue maternelle', 'the mother tongue'), ('une langue étrangère', 'a foreign language'), ('bilangue', 'b catch'), ('le rop', 'the dress'), (\"l'Afrique\", 'Africa'), (\"l'Amérique\", 'America'), (\"l'Asie\", 'Asia'), (\"l'Australie\", 'Australia'), ('la France', 'France'), ('Français, Française', 'French'), (\"l'Allemagne\", 'Germany'), ('Allemand, Allemande', 'German, German'), ('la Grande-Bretagne', 'Great Britain'), ('Britannique', 'British'), (\"l'Angleterre\", 'England'), ('Anglais-Anglaise', 'English'), (\"l'Italie\", 'Italy'), ('Italien, Italienne', 'Italian, Italian'), (\"l'Espagne\", 'Spain'), ('espagnol, espagnol', 'Spanish, Spanish'), ('le Portugal', 'Portugal'), ('portugais, portugaise', 'Portuguese'), ('la Belgique', 'Belgium'), ('Belge', 'Belgium'), ('les Pays-Bas', 'the Netherlands'), ('néerlandais, néerlandaise', 'Dutch'), ('la Hollande', 'Holland'), ('Hollandaise', 'Dutch'), ('le Luxembourg', 'Luxembourg'), ('luxembourgeois, luxembourgeoise', 'Luxembourgese'), (\"l'autriche\", 'Austria'), ('Autrichien, Autrichienne', 'Austrian, Austrian'), ('la Grèce', 'Grease'), ('Grec', 'Greek'), ('grecque', 'Greek'), (\"l'Irlande\", 'Ireland'), ('Irlandais, Irlandaise', 'Irish'), ('la Finlande', 'Finland'), ('Finlandais, Finlandaise', 'Finnish'), ('la Suède', 'Sweden'), ('suédois, suédoise', 'Swedish'), ('la Norvège', 'Norway'), ('Norvégien, Norvégienne', 'Norwegian'), ('le Danemark', 'Denmark'), ('Danois, Danoise', 'Danish, Danes'), ('Scandinave', 'Scandinavia'), ('la Suisse', 'Switzerland'), ('Suisse', 'Switzerland'), ('1, 1, Suisse', 'one, one, Switzerland'), ('Une Suisse S', 'A Swissess'), ('la Russie', 'Russia'), ('Russe', 'Russian'), ('la Pologne', 'Poland'), ('Polonais, Polonaise', 'Polish, Polish'), ('la République tchèque', 'the Czech Republic'), ('Check', 'Check'), ('la Slovaquie', 'Slovakia'), ('Slowhat', 'slow strip'), ('la Turquie', 'Turkey'), ('tük', 'Turkish'), ('Turc', 'Tur'), (\"l'Albanie\", 'Albania'), ('Albanais, Albanaises', 'Albanian Albanian'), ('la Bulgarie', 'Bulgaria'), ('vulgares', 'Bulgar'), ('la Roumanie', 'Romania'), ('Romain, Roumaine', 'Romanian'), ('la Hongrie', 'Hungary'), ('hongrois, hongroise', 'Hungarian'), (\"l'Estonie\", 'Estonia'), ('Estonia, Estonienne', 'Estonian'), ('la Lettonie', 'Lethony'), ('lèton, letonne', 'lethons, lethons'), ('la Lituanie', 'Lithuania'), ('lituaniens-lituaniennes', 'Lithuanian'), ('la Biélorussie', 'Belarus'), ('Bielorusse', 'Belarus'), (\"l'Ukraine\", 'Ukraine'), ('Ukrainiens-Ukrainiennes', 'Ukrainian'), ('la Croatie', 'Croatia'), ('Quoi?', 'What?'), ('la serbe', 'the Serbian'), ('Serbes', 'Serb'), ('la Slovénie', 'Slovenia'), ('Slovene', 'Slovenia'), ('la Bosnie-Herzégovine', 'Bosnia Herzegovina'), ('', 'pow'), ('les Etats-Unis', 'the United States'), ('les USA', 'the USA'), ('américain, américaine', 'American American'), ('le Canada', 'Canada'), ('Canadien, Canadienne', 'Canadian'), ('le Mexique', 'Mexico'), ('Mexicain, Mexicaine', 'Mexican, Mexican'), ('le Brésil', 'Brazil'), ('brésiliens, brésiliennes', 'Brazilian'), ('la Chine', 'China'), ('Chinois, Chinoise', 'Chinese'), ('le Japon', 'Japan'), ('Japonais, Japonaise', 'Japanese'), (\"l'Inde\", 'India'), ('Indiens, Indiennes', 'Indian Indian'), (\"l'Algérie\", 'Algeria'), ('Algériens, Algériennes', 'Algerian Algerian'), ('le marrote', 'ёл'), ('Marocain, Marocaine', 'Moroccan Moroccan'), ('la Tunisie', 'Tunisia'), ('Tunisien, Tunisienne', 'Tunisian, Tunisian'), ('le Maghreb', 'the Maghreb'), ('Maghrébin, Maghrébine', 'Maghreb, Maghrebine'), (\"l'Égypte\", 'Egypt'), ('égyptien, égyptienne', 'Egyptian, Egyptian')]\n"
     ]
    }
   ],
   "source": [
    "def sanitise(phrase):\n",
    "    # Strip whitespace then full stop\n",
    "    stripped = phrase.strip().strip(\".\")\n",
    "\n",
    "    # Often preceding proper nouns\n",
    "    exceptions = [\"The\", \"La\", \"Le\", \"L\", \"Les\"]\n",
    "\n",
    "    phrase_clean = None\n",
    "    # Un-capitalize phrases\n",
    "    # All uppers are usually errors\n",
    "    if stripped.isupper():\n",
    "        phrase_clean = stripped.lower()\n",
    "    # Title case is sometimes a noun, also check for empty string\n",
    "    # Also check it's not a question\n",
    "    elif (re.split(\" |'\", stripped)[0] in exceptions or not stripped.istitle()) and len(stripped) > 1 and not stripped[-1] == \"?\":\n",
    "        phrase_clean = stripped[0].lower() + stripped[1:]\n",
    "    else:\n",
    "        phrase_clean = stripped\n",
    "\n",
    "    return phrase_clean\n",
    "\n",
    "# Sanitise all outputs\n",
    "pairs = [ tuple(map(sanitise, phrase_pair)) for phrase_pair in zip(phrases_fr, phrases_en)]\n",
    "\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foolishly unzipping previously unzipped quantity\n",
    "# Putting it in a data frame\n",
    "data = pd.DataFrame(dict(zip([\"phrases_fr\", \"phrases_en\"], zip(*pairs))))\n",
    "\n",
    "# Saving to CSV\n",
    "CSV_title = re.sub(\",? \", \"_\", filename_title)\n",
    "data.to_csv(CSV_title + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Anki Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now have \"phrases_fr\", \"phrases_en\" and \"chunk_filename_list\", which is enough to create a deck\n",
    "random.seed(1)\n",
    "\n",
    "# Model without audio\n",
    "model_no_audio = genanki.Model(\n",
    "  random.randrange(1 << 30, 1 << 31),\n",
    "  'English/French without Audio',\n",
    "  fields=[\n",
    "    {'name': 'English'},\n",
    "    {'name': 'French'},\n",
    "  ],\n",
    "  templates=[\n",
    "    {\n",
    "      'name': 'Card',\n",
    "      'qfmt': '{{English}}',\n",
    "      'afmt': '{{FrontSide}}<hr id=\"answer\">{{French}}',\n",
    "    },\n",
    "    ])\n",
    "\n",
    "model_audio = genanki.Model(\n",
    "  random.randrange(1 << 30, 1 << 31),\n",
    "  'English/French with Audio',\n",
    "  fields=[\n",
    "    {'name': 'English'},\n",
    "    {'name': 'French'},\n",
    "    {'name': 'Audio'}\n",
    "  ],\n",
    "  templates=[\n",
    "    {\n",
    "      'name': 'Card',\n",
    "      'qfmt': '{{English}}',\n",
    "      'afmt': '{{FrontSide}}<hr id=\"answer\">{{French}}<br>{{Audio}}',\n",
    "    },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deck\n",
    "my_deck = genanki.Deck(\n",
    "  random.randrange(1 << 30, 1 << 31), # model_id\n",
    "  filename_title)\n",
    "\n",
    "# Loop through words\n",
    "for i, ((phrase_fr, phrase_en), chunk_filename) in enumerate(zip(pairs, chunk_filename_list)):\n",
    "    note = genanki.Note(model=model_audio, fields=[phrase_en, phrase_fr, \"[sound:{}]\".format(chunk_filename)])\n",
    "\n",
    "    my_deck.add_note(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create package\n",
    "my_package = genanki.Package(my_deck)\n",
    "my_package.media_files = chunk_filename_list\n",
    "\n",
    "my_package.write_to_file(filename_prefix + \".apkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
