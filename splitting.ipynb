{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulating audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "from pydub.playback import play\n",
    "\n",
    "# Modifying strings\n",
    "import re\n",
    "\n",
    "# Machine Learning Model\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose File\n",
    "filename_dir = \"french_audio/split_testing/\"\n",
    "filename = \"01.2 Nationality, Language, Country.mp3\"\n",
    "\n",
    "filename_prefix = filename[:4]\n",
    "filename_title = filename[4:-4]\n",
    "\n",
    "track = AudioSegment.from_mp3(filename_dir + filename)\n",
    "original_bitrate = mediainfo(filename_dir + filename)[\"bit_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters empirically tuned\n",
    "chunks = split_on_silence(track, min_silence_len=600, silence_thresh=-30, keep_silence=200)\n",
    "\n",
    "def chunk_filename(filename_prefix, i):\n",
    "    return filename_prefix + \"_\" + str(i) + \".mp3\"\n",
    "\n",
    "# Save split up audio\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.export(chunk_filename(filename_prefix, i).format(i), format=\"mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Apply to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561ca13d9fd94ae5bd8aaa21758e82bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phrases_fr = []\n",
    "phrases_en = []\n",
    "\n",
    "decode_options = {\"fp16\": True}\n",
    "\n",
    "num_chunks = len(chunks)\n",
    "for i in tqdm(range(num_chunks)):\n",
    "    # Ignoring the English title\n",
    "    if i > 1:\n",
    "        result_fr = model.transcribe(chunk_filename(filename_prefix, i), language=\"fr\", task=\"transcribe\", **decode_options)\n",
    "        result_en = model.transcribe(chunk_filename(filename_prefix, i), language=\"fr\", task=\"translate\", **decode_options)\n",
    "\n",
    "        phrases_fr.append(result_fr[\"text\"])\n",
    "        phrases_en.append(result_en[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('un passeport', 'a passport'), (\"les papiers d'identité.\", 'identity papers'), (\"une carte d'identité\", 'an identity card.'), ('un continent.', 'a continent.'), ('a bie!', 'a B'), ('une nation', 'a nation'), ('la nationalité', 'nationality'), ('fatima va demander la nationalité française.', 'fatima will ask French nationality.'), (\"être d'origine\", 'being of origin'), (\"c'est un français d'origine italienne.\", \"it's a Frenchman of Italian origin.\"), ('éTRANGER', 'stranger'), ('éTRANGER', 'foreign'), ('un étranger', 'a stranger'), ('une étrangère', 'a foreigner'), ('immigrés.', 'immigrate'), ('un immigré', 'an immigrant'), ('une immigrée', 'an immigrant'), (\"les immigrés ont parfois du mal à s'intégrer.\", 'immigrants sometimes have trouble integrating themselves.'), (\"l'immigration\", 'immigration'), ('émigrez.', 'emigrate'), ('une langue', 'a language'), ('la langue maternelle', 'the mother tongue'), ('une langue étrangère', 'a foreign language'), ('bILANGUE', 'be taking a breath'), ('le rop', 'the dress'), (\"l'Afrique\", 'africa'), (\"l'Amérique.\", 'america'), (\"l'Asie\", 'asia'), (\"l'Australie\", 'australia'), ('la France', 'france'), ('français, Française', 'french'), (\"l'Allemagne\", 'germany'), ('allemand, Allemande', 'german, German'), ('la Grande-Bretagne', 'great Britain'), ('britannique', 'british'), (\"l'Angleterre\", 'england'), ('anglais-Anglaise', 'english'), (\"l'Italie.\", 'italy'), ('italien, Italienne.', 'italian, Italian.'), (\"l'Espagne\", 'spain'), ('espagnol, espagnol.', 'spanish, Spanish'), ('le Portugal', 'portugal'), ('portugais, portugaise', 'portuguese'), ('la Belgique', 'belgium'), ('belge', 'belgium'), ('les Pays-Bas', 'the Netherlands'), ('néerlandais, néerlandaise', 'dutch'), ('la Hollande', 'holland'), ('hollandaise', 'dutch'), ('le Luxembourg', 'luxembourg'), ('luxembourgeois, luxembourgeoise', 'luxembourgese'), (\"l'autriche\", 'austria'), ('autrichien, Autrichienne', 'austrian, Austrian'), ('la Grèce', 'grease'), ('grec', 'greek'), ('grecque.', 'greek.'), (\"l'Irlande\", 'ireland'), ('irlandais, Irlandaise', 'irish'), ('la Finlande', 'finland'), ('finlandais, Finlandaise.', 'finnish'), ('la Suède', 'sweden'), ('suédois, suédoise', 'swedish'), ('la Norvège', 'norway'), ('norvégien, Norvégienne', 'norwegian'), ('le Danemark', 'denmark'), ('danois, Danoise', 'danish, Danes'), ('scandinave.', 'scandinavia'), ('la Suisse', 'switzerland'), ('suisse.', 'switzerland'), ('1, 1, Suisse', 'one, one, Switzerland.'), ('une Suisse S', 'a Swissess'), ('la Russie', 'russia'), ('russe', 'russian'), ('la Pologne.', 'poland'), ('polonais, Polonaise', 'polish, Polish'), ('la République tchèque', 'the Czech Republic'), ('check.', 'check'), ('la Slovaquie', 'slovakia'), ('slow-Vac', 'slovak'), ('la Turquie', 'turkey'), ('tÜK', 'turkish'), ('turc', 'tur.'), (\"l'Albanie\", 'albania'), ('albanais, Albanaises.', 'albanian Albanian'), ('la Bulgarie', 'bulgaria'), ('vulgares.', 'bulgar'), ('la Roumanie', 'romania'), ('romain, Roumaine', 'romanian'), ('la Hongrie', 'hungary'), ('hongrois, hongroise', 'hungarian'), (\"l'Estonie\", 'estonia'), ('estonia, Estonienne', 'estonian'), ('la Lettonie', 'lethony'), ('lettons, lettonnes.', 'lethons, lethons.'), ('la Lituanie', 'lithuania'), ('lituaniens-lituaniennes', 'lithuanian'), ('la Biélorussie', 'belarus'), ('bielorusse', 'belarus'), (\"l'Ukraine\", 'ukraine'), ('ukrainiens-Ukrainiennes', 'ukrainian'), ('la Croatie', 'croatia'), ('quoi?', 'what?'), ('la serbe', 'the Serbian...'), ('serbes', 'serb'), ('la Slovénie', 'slovenia'), ('slovene', 'slovenia'), ('la Bosnie-Herzégovine', 'bosnia Herzegovina'), ('b O S T N I A G', 'see u next time!'), ('les Etats-Unis', 'the United States'), ('les USA', 'the USA'), ('américain, américaine.', 'american American'), ('le Canada.', 'canada'), ('canadien, Canadienne', 'canadian'), ('le Mexique', 'mexico'), ('mexicain, Mexicaine.', 'mexican, Mexican.'), ('le Brésil', 'brazil'), ('brésiliens, brésiliennes.', 'brazilian'), ('la Chine', 'china'), ('chinois, Chinoise.', 'chinese'), ('le Japon', 'japan'), ('japonais, Japonaise', 'japanese'), (\"l'Inde\", 'india'), ('indiens, Indiennes', 'indian Indian'), (\"l'Algérie\", 'algeria'), ('algériens, Algériennes', 'algerian Algerian'), ('le marrote', 'the Dark Side'), ('marocain, Marocaine.', 'moroccan Moroccan'), ('la Tunisie', 'tunisia'), ('tunisien, Tunisienne.', 'tunisian, Tunisian'), ('le Maghreb', 'the Maghreb'), ('maghrébin, Maghrébine.', 'maghreb, Maghrebine'), (\"l'Égypte\", 'egypt'), ('égyptien, égyptienne.', 'egyptian, Egyptian')]\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "for i, (phrase_fr, phrase_en) in enumerate(zip(phrases_fr, phrases_en)):\n",
    "    # Remove trailing whitespace\n",
    "    phrase_fr_strip = phrase_fr.lstrip()\n",
    "    phrase_en_strip = phrase_en.lstrip()\n",
    "\n",
    "    # Un-capitalize phrases\n",
    "    # TODO Code in logic that takes into account final full stop\n",
    "    # TODO Deal with ALL CAPS case and make it all lower case\n",
    "    phrase_fr_clean = phrase_fr_strip[0].lower() + phrase_fr_strip[1:]\n",
    "    phrase_en_clean = phrase_en_strip[0].lower() + phrase_en_strip[1:]\n",
    "\n",
    "    pairs.append((phrase_fr_clean, phrase_en_clean))\n",
    "\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
